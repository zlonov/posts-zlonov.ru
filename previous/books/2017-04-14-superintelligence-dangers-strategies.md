---
layout: zlonov/old_post
title: "Искусственный интеллект. Этапы. Угрозы. Стратегии"
date: 2017-04-14
category: book
featured-image: /books/superintelligence-dangers-strategies.jpg
AllowComments: true
tags: ['>700 страниц', 'искусственный интеллект', 'сверхразум', 'книга']
book_author: "Ник Бостром"
book_ISBN: 978-5-00057-810-0
book_number_of_pages: 760
book_litres_link: https://www.litres.ru/nik-bostrom/iskusstvennyy-intellekt-etapy-ugrozy-strategii/?lfrom=13913266
book_mif_link: http://www.mann-ivanov-ferber.ru/books/iskusstvennyj-intellekt/
excerpt: Отличная книга для первичного погружения в тему Больших Данных. Содержит описание интересных практических кейсов и общие базовые принципы. В книге нет математики и технической информации, читается легко даже при отсутствии специальной подготовки.
LinkedLinks:
  - title: 'Исследователи из Оксфорда и Google DeepMind: Скорее всего, ИИ уничтожит человечество'
    url: https://safe.cnews.ru/news/top/2022-09-16_issledovateli_iz_oksforda
---
Книга <strong><a href="https://www.litres.ru/nik-bostrom/iskusstvennyy-intellekt-etapy-ugrozy-strategii/">Искусственный интеллект. Этапы. Угрозы. Стратегии</a></strong>, пожалуй, одна из самых непростых для прочтения для меня за последнее время.

Во-первых, она очень объёмная и с огромным количеством сносок, некоторые из которых тянут на самостоятельные если не главы, то подразделы уж точно.

Во-вторых, автор, идя последовательно от одной проблемы, связанной с созданием искусственного интеллекта, к последующей постоянно не показывает наиболее оправданный вариант решения задачи этого этапа, а лишь обозначает возможные существующие и даже предполагаемые пути и тут же переходит к следующему этапу (<em>"...И снова речь не о том, что все будет именно так."</em>). В итоге, на каждом новом витке нужно держать в голове все ворохи предыдущих гипотез и умозаключений, представленных к тому же без строгих как доказательств, так и опровержений.

Предисловие к русскому переводу книги, написанное Евгением Касперским, начинает приведённой к месту цитатой из Стругацких:
>…У меня есть один знакомый, — сказал Эдик. — Он утверждает, будто человек — промежуточное звено, необходимое природе для создания венца творения: рюмки коньяка с ломтиком лимона.\
_Аркадий и Борис Стругацкие. Понедельник начинается в субботу_

Собственно, про венец творения в виде сверхразума (определён в книге так: «_Сверхразум - это любой интеллект, значительно превосходящий когнитивные возможности человека фактически в любых областях_») и идёт речь в книге Ника Бострома.

Существенную часть книги занимает описание всех тех опасностей, которые может повлечь за собой появление/создание сверхразума. Про это говорится даже чаще, чем про преимущества от его появления, впрочем, делается это весьма убедительно:

> Как правило, человек как действующее лицо останавливается в нерешительности, если предполагаемые действия необратимы, нарушают устоявшиеся нормы или не имеют исторических прецедентов. Сверхразуму как действующей силе ситуация может показаться вполне однозначной, и его не остановит неопределенность результата, если он решит попытаться воспользоваться своим решающим стратегическим преимуществом для укрепления собственного господствующего положения.

и с обоснованием, исходя из различных изначальных предпосылок:

> Можно допустить альтернативный вариант развития: сверхразум будет настолько уверен в собственной мощи и неуязвимости, что не станет воспринимать наш вид в качестве угрозы своему существованию, — таким образом, у человечества появляется шанс не превратиться в элементарную мишень и уйти из-под прямого удара. И в этом случае закат человеческой цивилизации неизбежен, но случится он по другой причине: будет разрушена наша среда обитания. Сверхразум начнет внедрять нанотехнологические фабрики и наноассемблеры, и в результате глобальных строительных проектов вся поверхность планеты быстро — буквально за считаные дни или недели — покроется солнечными батареями, ядерными реакторами, корпусами суперкомпьютеров с торчащими башнями охлаждения, стапелями для запуска космических кораблей и прочими сооружениями.

Далее на протяжении нескольких глав автор рассматривает вопросы, связанные с тем или иным способом противодействия сверхразуму, и каждый раз находит сценарий, который таки может привести к печальным последствиям:

> Вероломный ход может вытекать из стратегического решения: играть по правилам, тянуть время, пока еще слаб, накапливать силы и нанести удар позже, — но я не стал бы интерпретировать эту модель столь узко. Например, ИИ вполне способен отказаться от мысли хитрить, поскольку совершенно равнодушен к идее собирания сил, процветания и даже выживания. Он просчитает, что после его уничтожения программисты создадут новый ИИ, несколько иной конфигурации, но с похожими служебными функциями. В этом случае оригинальному ИИ будет безразлична собственная гибель, поскольку он знает, что его конечные цели все равно будут реализованы в будущем. Он может даже выбрать стратегию демонстративного и вызывающе неправильного функционирования в определенных критически важных для него или людей областях. В результате, приступая к следующей разработке, программисты будут считать, что получили от прежней системы важную информацию об ИИ, и начнут больше доверять новой версии, увеличив тем самым шансы на достижение целей оригинального ИИ, к этому времени уже не существующего. Может существовать множество стратегических факторов, оказывающих влияние на действия усовершенствованного ИИ, и было бы высокомерием полагать, будто мы в состоянии оценить их все, особенно когда речь идет об ИИ, обладающем сверхмощью в области выработки стратегии.

или так:

> Предположим, что конечная цель системы — «доставлять удовольствие организатору проекта». Вначале единственным доступным для ИИ способом достижения этой цели является такое поведение, которого ожидает от него сам организатор проекта. Интеллектуальная система дает полезные советы, обнаруживает дивный характер, зарабатывает деньги. Чем сильнее становится ИИ, тем больше его действия вызывают чувство удовлетворения организатора, — и все идет в соответствии с планом. Идет до тех пор, пока система не станет настолько разумной, что наконец поймет: стоящую перед ней задачу можно выполнить самым полным и надежным способом, если имплантировать электроды в центры удовольствия головного мозга организатора, что гарантированно сделает его более чем счастливым.

Порочной реализации изначально позитивных и разумных целей, которые можно поставить перед сверхразумом, Ник Бостром уделяет отдельное внимание:

> Конечная цель: сделай так, чтобы я всегда улыбался.\
Порочная реализация: поразить лицевой нерв, что приведет к параличу мимической мускулатуры, — тебе обеспечена вечно сияющая улыбка.

> Конечная цель: сделай так, чтобы я всегда улыбался, но обойдись без прямого воздействия на лицевой нерв.\
Порочная реализация: стимулировать двигательные зоны коры головного мозга, отвечающие за функции лицевого нерва, иннервирующего мимическую мускулатуру, — тебе обеспечена вечно сияющая улыбка.

> Конечная цель: сделай нас счастливыми.\
Порочная реализация: имплантировать электроды в центры удовольствия головного мозга.

> Конечная цель: действовать так, чтобы избежать впоследствии уколов совести.\
Порочная реализация: отключить соответствующий когнитивный модуль, то есть те зоны коры головного мозга, которые отвечают за чувство вины.

И как резюме:

> Примеры порочной реализации показывают: существует множество конечных целей, которые на первый взгляд кажутся разумными, а их реализация представляется вполне безопасной, но при детальном рассмотрении они могут иметь совершенно неожиданные последствия. Если сверхразум, имеющий какую-то из подобных целей, приобретет решающее стратегическое преимущество, то для человечества игра будет закончена.

Приведу ещё несколько показательных цитат, связанных с рассуждениями над возможностями человека каким-либо образом изолировать сверхразум и тем самым минимизировать вероятность экзистенциальной катастрофы:

> Наивно полагать, будто агент, не имеющий исполнительных механизмов, не в силах воздействовать на внешний мир. Даже не располагая такими приспособлениями, сверхразум способен генерировать радиоволны просто «силой мысли» (то есть определенным образом перемещать электроны в своих микросхемах). Это слабое звено можно устранить, заключив систему в клетку Фарадея, — но остается лишь догадываться, о скольких уязвимых местах еще мы не знаем. Всякий раз, услышав о неожиданно обнаруженном дефекте в, казалось бы, неприступной системе безопасности, следует насторожиться.


> Это далеко не простой вопрос: брать под сомнение то, в чем твердо уверен. Но если мы не научимся это делать, то потеряем шанс заблаговременно принять соответствующие меры — и тогда прощай заветная мечта об освоении космоса. Пожалуй, можно было бы поговорить в пользу такого принципа: если некто в прошлом каждый раз (N случаев) был убежден в абсолютной защищенности своей системы, но всякий раз оказывалось, что он ошибался, то в очередном случае рассуждая об ее безопасности, он не вправе присваивать вероятность, превышающую 1/(N + 1).


> Цель информационной изоляции состоит в ограничении информационных потоков, исходящих из «песочницы». Мы уже видели, насколько небезопасной может быть идея предоставить сверхразуму доступ в интернет и позволить ему передавать сообщения вовне: даже лишенный физических исполнительных механизмов, он в состоянии, использовав канал вывода информации, добиваться от людей, чтобы они выполнили его указания. Поэтому заведомым условием метода информационной изоляции ИИ является отказ в доступе к коммуникационным сетям.


> В одном неофициальном эксперименте в роли ИИ выступил человек — естественно, обладающий незаурядным интеллектом. В роль охранника, которого каждый раз исполняли разные люди, входило ни при каком случае не выпускать «искусственного интелекта» из комнаты. «Искусственному интеллекту» дали всего два часа на то, чтобы убедить своего сторожа отпустить его на свободу, причем разрешили вступать в общение с ним лишь посредством текстовых сообщений. В трех случаях из пяти «искусственному интеллекту» удавалось сбегать. Если смог человек, то сможет и сверхразум. (Обратное, конечно, неверно. Даже если перед сверхразумом будет стоять более трудная задача — возможно, его охранники будут сильнее мотивированы, чем люди в ходе упомянутого эксперимента, — сверхразум справится там, где потерпит поражение человек.)


> ...не стоит переоценивать безопасность системы. Психические образы легко заменяются на визуальные с помощью графической информации. Более того, вспомним, какое воздействие на человека оказывают книги — притом что книга, насколько мы знаем, не вступает в диалог со своим читателем.

В качестве некоего резюме отозвался бы о книге как об энциклопедии, посвящённой теориям, гипотезам и исследованиям, так или иначе связанным с искусственным интеллектом.

Рассмотрены самые разные аспекты, вопросы, даны обзоры существующих наработок и приведён исчерпывающий список литературы по каждому из аспектов. Встретив в одной из сносок фразу "подробное обсуждение этой темы выходит за рамки данной книги" невольно улыбнулся - настолько для только знакомящегося более глубоко с обозначенной предметной областью труд объёмен и, как уже отметил, энциклопедичен.

В книге нет чётких ответов, а лишь постановка вопросов и "рассуждения на тему". Но вопросы таковы, что точных ответов на них пока и быть не может.

> Мы заблудились в непроходимой чаще стратегической сложности, которую окутывает плотный туман неопределенности. Хотя многие элементы окружающего нас пейзажа разглядеть удалось, но отдельные детали и взаимосвязи остаются загадкой. Кроме того, могут быть другие факторы, о которых мы пока даже не задумываемся. Что нам остается в такой ситуации?

Далее представлены цитаты, которые выписал себе при прочтении, сгруппированные по разделам:
<ul>
 	<li><strong><a id="menu"></a><a href="#humor">Шутки и остроты из книги</a></strong></li>
 	<li><a href="#facts"><strong>Интересные факты из книги</strong></a></li>
 	<li><a href="#2read"><strong>Встречающие по тексту и в аннотации интересные ссылки и рекомендации к прочтению</strong></a></li>
 	<li><a href="#other"><strong>Другие выборочные цитаты и определения из книги</strong></a></li>
</ul>

<hr />

<h2><a id="humor"></a>Шутки и остроты из книги</h2>
<ul>
 	<li>Про то, почему прогнозы часто даются на 20 и более лет вперёд: «двадцатилетний срок чаще всего близок к средней продолжительности оставшейся профессиональной деятельности прогнозиста, что уменьшает репутационный риск, связанный с его дерзким предсказанием».</li>
 	<li>Цицерон в трактате «О прорицании» (De divinatione) заметил, что «нет такого абсурда, который нельзя было бы найти в книгах философов».</li>
 	<li>Остается лишь согласиться с Джоном Маккарти, когда-то посетовавшим, что «стоит системе нормально начать работать, как ее сразу перестают называть искусственным интеллектом».</li>
 	<li>Во времена расцвета популярной научной фантастики, довольно дешевого свойства, обложки журналов пестрели картинками, на которых очередное инопланетное чудовище — в народе более известное как «пучеглазый монстр» — в очередной раз куда-то тащило очередную красотку в обязательно задранном платье — причем красотка была нашей, земной, женщиной. Похоже, все художники уверовали, что негуманоидные пришельцы с совершенно иной эволюционной историей непременно должны испытывать сексуальное влечение к прекрасным представительницам человеческого рода. &lt;…&gt; Скорее всего, художники, изображавшие все это, даже не задавались вопросом, а будет ли вообще гигантский жук чувствителен к прелестям наших женщин. Ведь по представлениям художников любая полуобнаженная женщина просто по определению сексуально привлекательна, то есть испытывать к ней желание являлось неотъемлемой чертой мужественных представителей человеческого рода. Все художническое внимание было направлено на задранное или порванное платье, меньше всего их заботило, как устроено сознание гигантских насекомообразных. И это составляло главную ошибку художников. Не будь одежды изодраны, — думали они, — женщины выглядели бы не столь соблазнительно для пучеглазых монстров. Жаль только, сами пришельцы так и не взяли этого в толк.</li>
 	<li>Если обучившаяся модифицировать самою себя думающая машина вдруг испытает острое желание стать глупой, то довольно быстро она перестанет быть интеллектуальной системой.</li>
 	<li>Примером такой стратегии может быть поведение аcцидии: она свободно плавает будучи личинкой, но, повзрослев, находит подходящий камень, к которому прикрепляется навсегда. Обретя свое место, асцидия перестает нуждаться в сложной системе обработки информации и начинает поедать собственный мозг (в частности, головной ганглий). Такие же процессы происходят — что мы можем наблюдать — и с некоторыми учеными, когда они, став штатными профессорами, отдают себя в полное владение университетам.</li>
 	<li>Говоря языком программистов, когда ставишь одно дырявое ведро в другое дырявое ведро — вода все равно вытекает наружу.</li>
 	<li>Каждое новое поколение ученых пытается избавиться от онтологических категорий, введенных когда-то их предшественниками (кто сейчас помнит такие понятия, как «флогистон», «сила жизни» и «абсолютная одновременность»?)</li>
 	<li>Может быть, природа и великий экспериментатор, но на свои опыты она никогда не получит одобрения у совета по этике, поскольку постоянно нарушает Хельсинкскую декларацию со всеми ее этическими нормами.</li>
 	<li>В ходе недавних опросов профессиональных философов была выявлена доля респондентов, которые «поддерживают или склоняются к поддержке» тех или иных теорий. В области нормативной этики: деонтология — 25,9%; консеквенциализм — 23,6%; этика добродетели —18,2%. В области метаэтики: моральный реализм — 56,4%; моральный антиреализм — 27,7%. В области моральных суждений: когнитивизм — 65,7%; нонкогнитивизм — 17,0%. Ни одна этическая теория не получила признания большинства философов, таким образом, можно считать, что большинство неправо.</li>
 	<li>...если область станет модной, в нее, несомненно, потянутся бездари и психи.</li>
</ul>
<p style="text-align: right;"><a href="#menu"><em>Вверх</em></a>

<h2><a id="facts"></a>Интересные факты из книги</h2>
<ul>
 	<li>Если проанализировать сегодняшнюю численность людей на Земле, то выяснится, что в среднем она увеличивается на 1 млн человек за полторы недели.</li>
 	<li>Летом 1956 года в Дартмутском колледже собрались на двухмесячный семинар десять ученых, объединенных общим интересом к нейронным сетям, теории автоматов и исследованию интеллекта. Время проведения Дартмутского семинара обычно считают точкой отсчета новой области науки — изучения искусственного интеллекта. Большинство его участников позднее будут признаны основоположниками этого направления.</li>
 	<li>Хотя простые модели нейронных сетей были известны с конца 1950-х годов, ренессанс в этой области начался после создания метода обратного распространения ошибки, который позволил обучать многослойные нейронные сети. Основной алгоритм был описан в 1969 году Артуром Брайсоном и Юй-Чи Хо как многошаговый метод динамической оптимизации. Применить его к нейронным сетям предложил в 1974 году Пол Вербос, но признание у научного сообщества этот метод получил лишь в 1986 году после работы Дэвида Румельхарта, Джеффри Хинтона и Рональда Уильямса.</li>
 	<li>Caenorhabditis elegans — прозрачный круглый червь (нематода), длиной около одного миллиметра и имеющий всего 302 нейрона. Это скромное неприметное создание служит для науки модельным организмом. Его коннектом, то есть полное описание структуры связей его нейронов, известен с середины 1980-х годов.</li>
 	<li>Классический пример, относящийся к уже далекому 1975 году, когда Стивен Домпье обнаружил, что его Altair 8800 (один из первых персональных компьютеров, примитивный и не экранированный) вызывает помехи в стоящем рядом радиоприемнике. Тогда он написал программу, управляющую электромагнитными разрядами, и с помощью приемника проиграл на своем Altair мелодию битловской Fool on the Hill («Дурак на холме»). Присутствовавший на демонстрации молодой Билл Гейтс был восхищен и заинтригован увиденным. Сегодня есть планы создания микропроцессоров со встроенным Wi-Fi-передатчиком.</li>
 	<li>Три закона робототехники по Азимову:
<ol>
 	<li>Робот не может причинить вред человеку или своим бездействием допустить, чтобы человеку был причинен вред.</li>
 	<li>Робот должен повиноваться всем приказам, которые дает человек, кроме тех случаев, когда эти приказы противоречат первому закону;</li>
 	<li>Робот должен заботиться о своей безопасности в той мере, в которой это не противоречит первому и второму законам</li>
</ol>
</li>
 	<li>Позднее был добавлен «нулевой» закон: робот не может причинить вред человечеству или своим бездействием допустить, чтобы человечеству был причинен вред.</li>
 	<li>&lt;...&gt;ПО, непосредственно замкнутое на достаточно мощные исполнительные механизмы, скажем, системы раннего предупреждения о ракетном нападении, напрямую соединенные с ядерными боеголовками или передающие информацию офицерам, уполномоченным на нанесение ядерного удара. Ошибки в его работе способны привести к абсолютно рискованным ситуациям. В истории человечества это происходило минимум дважды.
<ul>
 	<li>Первый случай: 9 ноября 1979 года в результате компьютерного сбоя Объединенное командование воздушно-космической обороны Североамериканского континента получило ложный сигнал о начале полномасштабного нападения СССР на США. Немедленно началась подготовка ответного удара, но данные с радарных систем раннего предупреждения показали, что ни одной ракеты со стороны СССР запущено не было.</li>
 	<li>Второй случай: 26 сентября 1983 года ошибочно сработала «Око» — советская спутниковая система обнаружения стартов межконтинентальных баллистических ракет с континентальной части США, — сообщив о ракетном ударе со стороны Соединенных Штатов. Оперативный дежурный командного пункта подполковник Станислав Петров правильно определил, что эта тревога ложная, — практически он один предотвратил ядерную войну. Вряд ли она привела бы к исчезновению человечества, даже если был бы задействован весь ядерный потенциал, имевшийся у всех стран на пике холодной войны, но, безусловно, вызвала бы неисчислимые смерти и страдания и крах современной цивилизации.</li>
</ul>
</li>
 	<li>При наличии довольно большой — ограниченной, но физически невозможной — вычислительной мощности было бы возможно получить универсальный сверхразум даже на базе имеющихся сейчас алгоритмов. Но для достижения нужного уровня вычислительной мощности будет недостаточно соблюдения закона Мура даже в течение еще ста лет.</li>
 	<li>Когда лошади морально устарели в качестве средства передвижения, многие были проданы на бойню и пошли на собачий корм, костную муку, кожу и клей. У этих животных не было альтернативного источника использования, который окупил бы их содержание. В США в 1915 году было около двадцати шести миллионов лошадей. К началу 1950-х годов осталось два миллиона.
<ul>
 	<li>После сокращения американской популяции лошадей к началу 1950-х годов до двух миллионов особей она начала быстро восстанавливаться и, по недавним статистическим данным, выросла до десяти миллионов голов. Этот рост обусловлен не спросом на лошадей в сельском хозяйстве или на транспорте, а скорее тем, что в результате экономического роста все больше американцев могут позволить себе следовать моде на владение лошадьми для удовольствия.</li>
</ul>
</li>
 	<li>Чтобы 7 млрд человек получали годовую пенсию в размере 90 000 долларов, потребуется 630 трлн долларов в год, что в десять раз превышает величину современного мирового ВВП. По имеющимся данным, за последние 100 лет он вырос в 19 раз — с 2 млрд долларов в 1900 году до 37 млрд долларов в 2000-м (в международных долларах 1900 года). Поэтому если темпы роста, которые мы видели в прошлом веке, сохранятся на протяжении следующих двухсот лет, а население планеты не вырастет, обеспечение каждого годовой пенсией в 90 000 долларов будет стоить всего 3% мирового ВВП. Благодаря взрывному развитию искусственного интеллекта такой рост может случиться гораздо раньше.</li>
 	<li>...публичное уничтожение большого количества накопленного имущества индейцами квакиутл во время демонстративного обмена дарами между племенами, когда происходило своего рода соревнование вождей за максимальное влияние и авторитет, только орудием борьбы выступало их богатство, — эта традиционная церемония называлась «потлач». Современными аналогами потлача можно считать рекордно высокие небоскребы, очень крупные и непомерно дорогие яхты, а также попытки строительства ракет для полета на Луну.</li>
 	<li>Вероятно, мы забыли ту цветовую мешанину, которую наблюдали в раннем младенчестве, когда мозг еще не научился интерпретировать поступающую в него визуальную информацию.</li>
 	<li>Хельсинкская декларация (Declaration of Helsinki) — набор этических принципов для медицинского сообщества, касающихся экспериментов на людях; разработана Всемирной медицинской ассоциацией в 1964 году.</li>
 	<li>В Париже XVI века популярным действом было сжигание кошек.</li>
 	<li>Из истории нам известно множество примеров, когда методы ИИ брали начало в области нейробиологии и даже обычной биологии:
<ul>
 	<li>Например, нейрон Маккаллока–Питтса, перцептроны, или персептроны, и другие искусственные нейроны и нейронные сети появились благодаря исследованиям в области нейроанатомии;</li>
 	<li>обучение с подкреплением инспирировано бихевиоризмом;</li>
 	<li>генетические алгоритмы — эволюционной теорией;</li>
 	<li>архитектура поведенческих модулей и перцепционная иерархия — теориями когнитивистики о планировании движений и чувственном восприятии;</li>
 	<li>искусственные иммунные системы — теоретической иммунологией;</li>
 	<li>роевой интеллект — экологией колоний насекомых и других самоорганизующихся систем;</li>
 	<li>реактивный и основанный на поведении контроль в робототехнике — исследованиями механизма передвижения животных.</li>
</ul>
</li>
</ul>
<p style="text-align: right;"><a href="#menu"><em>Вверх</em></a>

<h2><a id="2read"></a>Встречающие по тексту и в аннотации интересные ссылки и рекомендации к прочтению</h2>
<ul>
 	<li>Себастиан Трун и Питер Норвиг подготовили в Стэнфордском университете на осень 2011 года бесплатный онлайновый вводный курс по искусственному интеллекту.</li>
 	<li>Даже хакеры-люди способны писать небольшие и внешне невинные программы, способные делать совершенно неожиданные вещи. (Примеры можно найти, просмотрев список победителей Международного конкурса на самый запутанный код на языке Cи.)</li>
 	<li>Так всех нас в трусов превращает мысль
И вянет, как цветок, решимость наша
В бесплодье умственного тупика.
Так погибают замыслы с размахом,
Вначале обещавшие успех,
От долгих отлагательств.Уильям Шекспир. Гамлет (акт 3, сцена 1).</li>
</ul>
<p style="text-align: right;"><a href="#menu"><em>Вверх</em></a>

<h2><a id="other"></a>Другие выборочные цитаты и определения из книги</h2>
<ul>
 	<li>Чтобы справиться с комбинаторным взрывом, нужны алгоритмы, способные анализировать структуру целевой области и использовать преимущества накопленного знания за счет эвристического поиска, долгосрочного планирования и свободных абстрактных представлений.</li>
 	<li>Можно только предполагать, почему машине трудно достичь человеческого уровня в восприятии окружающей действительности, регуляции двигательных функций, здравом смысле и понимании языка. Одна из причин заключается в том, что в нашем мозгу имеется специальный механизм, управляющий этими свойствами, — достигшие в процессе эволюции совершенства нейронные структуры. Логическое мышление и навыки вроде игры в шахматы, в отличие от перечисленных выше способностей, не столь естественны, и потому при решении этих задач мы вынуждены полагаться на ограниченные когнитивные ресурсы общего назначения. Возможно, для вычислений и явно выраженных логических рассуждений наш мозг запускает что-то похожее на «виртуальную машину» — медленный и громоздкий психический симулятор универсального компьютера. Если наше предположение верно, то тогда получается забавная вещь: не КИИ моделирует человеческое мышление, а как раз наоборот — логически мыслящий человек симулирует программу ИИ.</li>
 	<li>Для людей и управляемых людьми организаций также характерен процесс принятия решений, не подразумевающий стремления максимизировать функцию полезности. Например, могут оказывать влияние такие обстоятельства: природная склонность человека стараться избегать риска; принцип разумной достаточности, главным критерием которого является поиск и принятие удовлетворительного варианта; наличие этических ограничений, запрещающих совершать определенные действия независимо от степени их желательности. Принимающие решения люди часто действуют сообразно своим личным качествам или социальной роли, а не стремлению максимизировать­ вероятность достижения той или иной цели. И опять — это не обязательно будет присуще искусственным агентам.</li>
 	<li>Тезис об ортогональности гласит (с некоторыми исключениями), что можно комбинировать любой уровень интеллекта с любой целью, поскольку интеллект и конечные цели представляют собой ортогональные, то есть независимые, переменные. Тезис об инструментальной конвергенции гласит, что сверхразумные действующие силы, или агенты, — при самом широком разнообразии своих конечных целей — тем не менее будут преследовать сходные промежуточные цели, поскольку на это у всех агентов будут одинаковые инструментальные причины.</li>
 	<li>Если с наличием интеллекта и знаний связаны определенные затраты — например, времени и усилий, потраченных на их приобретение, или дополнительные требования к хранению и обработке информации, — то агент может предпочесть меньше знать и быть менее интеллектуальным.</li>
 	<li>Если человек по лени своей или легкомыслию не предпримет целенаправленных усилий, то первый сверхразум будет иметь довольно случайный набор примитивных окончательных задач.</li>
 	<li>Уже в следующий момент — сразу после рождения обмана — ИИ может решить стереть следы своих мятежных мыслей. Поэтому очень важно, чтобы «растяжки» работали постоянно. Кроме того, было бы полезно иметь «черный ящик» вроде тех, которые используются в самолетах, где могли бы храниться следы всех действий ИИ (включая точное время введения информации с клавиатуры программистами), чтобы после автоматического отключения системы их можно было проследить и проанализировать. Храниться информация может на устройстве с возможностью лишь однократной записи и многократного чтения.</li>
 	<li>Правда, для некоторых типов архитектуры ИИ осуществление такого контроля [имеется в виду контроль семантической информации] может быть невозможно. (Например, непрозрачными являются некоторые нейронные сети, поскольку информация в них представляется целостно и такими способами, которые не всегда соответствуют человеческим понятиям.) По этой причине лучше избегать использовать такие архитектуры.</li>
 	<li>Консеквенциализм (consequentialism) — этическая теория, согласно которой правильность или неправильность действий оценивается с точки зрения того, каковы их результаты или последствия.</li>
 	<li>Бертран Рассел, много лет трудившийся над основами математики, как-то заметил: «…Степень нечеткости не осознается вплоть до попытки нечто прояснить, а все точное столь далеко от всего того, о чем мы обычно мыслим, что нельзя и на мгновение предположить, что же мы на самом деле имеем в виду, когда выражаем наши мысли». Трудно найти лучшего комментария к проблемам, относящимся к методу точной спецификации.</li>
 	<li>Заметим, что синглтон является чистой абстракцией. Режим при нем может быть самым разным: демократия; тирания; единодержавие, то есть абсолютизм ИИ; набор жестких общемировых законов, подлежащих безотказному исполнению. Иначе говоря, синглтон — это некий орган, способный в одиночку решать все крупные проблемы общемирового масштаба. Он может нести в себе — но совсем не обязательно — характеристики знакомых нам форм человеческого управления.</li>
 	<li>При желании можно вообразить разные варианты, например: синглтон, чье существование вообще невидимо (когда сверхразум обладает настолько развитыми технологиями и проницательностью, что может незаметно для людей управлять всем миром); синглтон, добровольно установивший жесткие самоограничения своей власти (пунктуально соблюдает международные нормы и договоры); синглтон-либертарианец. Какова вероятность возникновения синглтона того или иного типа — вопрос скорее умозрительный, пока не будет проверено на опыте. Однако почему бы не дать волю фантазии хотя бы на концептуальном уровне: хороший синглтон, плохой синглтон, дерзкий синглтон, вздорный синглтон, приторно-любезный синглтон, деспотичный синглтон с признаками паранойи, вопящий самодур синглтон, порою напоминающий разбушевавшуюся природную стихию.</li>
 	<li>В крайнем случае можно представить высокоразвитое с технологической точки зрения общество, состоящее из множества сложных систем, в том числе гораздо более сложных и интеллектуальных, чем все, что существует на планете сегодня, — общество, совершенно лишенное кого-либо, кто обладал бы сознанием или чье благополучие имело бы какое-либо моральное значение. В некотором смысле это было бы необитаемое общество. Общество экономических и технологических чудес, никому не приносящих пользы. Диснейленд без детей.</li>
 	<li>Чтобы два агента имели одинаковые конечные цели (одинаковые в прямом смысле этого слова — то есть «одни и те же»), эти цели должны совпадать в своих дейктических (то есть указательных) элементах. Если Боб эгоист, то имитация Боба тоже будет эгоистом. Тем не менее цели их не одинаковые, поскольку Боба заботит благополучие Боба, а имитацию Боба — благополучие имитации Боба.</li>
 	<li>Обучение меняет не саму цель, а представления ИИ об этой цели.</li>
 	<li>Возьмем, например, невероятную легкость, с которой современные программисты создают агентов для обучения с подкреплением и применяют к ним негативные раздражители. Ежедневно создается бесчисленное количество таких агентов, причем не только в научных лабораториях, но и в многочисленных фирмах, где разрабатываются разные приложения и создаются компьютерные игры, содержащие множество­ сложных персонажей. Предположительно, эти агенты еще слишком примитивны, чтобы претендовать на какой-то моральный статус. Но можем ли мы быть уверены на все сто процентов? И еще одно важное замечание: можем мы быть уверены, что узнаем, в какой момент следует остановиться, чтобы программы не начали испытывать страдания?</li>
 	<li>Принцип эпистемологического превосходства. Будущий сверхразум занимает эпистемологически более высокий наблюдательный пункт: его убеждения (видимо, относительно большинства вопросов) с большей вероятностью окажутся истинными, чем наши. Поэтому при любых возможных обстоятельствах следует полагаться на его мнение.</li>
 	<li>Как можно вознаградить мертвых? В голову приходят несколько вариантов. Во-первых, организация мемориальных торжеств и создание монументов, которые могли бы считаться наградой в том случае, если люди желали посмертной славы. Во-вторых, имена умерших могли бы быть увековечены в таких сферах, как культура, искусство, архитектура и природа. В-третьих, большинство людей беспокоит судьба их потомков, поэтому специальные привилегии можно даровать их детям и внукам. В-четвертых, сверхразум был бы готов создать сравнительно правдоподобные имитационные модели умерших — модели, обладающие разумом и напоминающие­ свои оригиналы настолько, что ушедшие могли бы считаться воскресшими. Скорее всего, последний вариант достижим лишь для тех, кто после смерти был подвергнут криогенной заморозке, но, может быть, сверхразуму не составит труда сделать что-то подобное и на основании других сохранившихся материальных следов умершего человека: личная переписка, публикации, аудиовизуальные материалы, цифровые записи, а также воспоминания живущих. Кроме того, сверхразум найдет и другие возможности, для нас не столь очевидные.</li>
 	<li>Родственный этому тип аргументации сводится к тому, что нам следует — какая жестокость! — приветствовать мелкие и средние катастрофы на том основании, что они вскроют наши уязвимые места и заставят принять меры предосторожности, снижающие вероятность экзистенциальной катастрофы. Идея состоит в том, что мелкие и средние катастрофы служат своего рода прививкой: сталкиваясь с относительно слабой угрозой, цивилизация вырабатывает иммунитет, благодаря которому сможет справиться с потенциально губительным вариантом той же самой угрозы.</li>
 	<li>Если увеличивать размер компьютера, в конечном счете столкнешься с релятивистскими ограничениями из-за задержек в передаче информации между различными его частями — сигналы не могут передаваться быстрее скорости света. Если сжимать компьютер, то столкнешься с пределами миниатюризации на квантовом уровне. Если увеличивать плотность компьютера, упрешься в границы черной дыры. Однако нужно признать, что есть вероятность открытия новых физических свойств, которые когда-нибудь позволят обойти эти ограничения.</li>
 	<li>Перед лицом перспективы взрывного развития интеллекта мы похожи на детей, играющих с бомбой. Именно таков разрыв между мощностью нашей игрушки и незрелостью нашего поведения. Сверхразум — это вызов, ответить на который мы не готовы сегодня, и не будем готовы еще долгое время. Мы понятия не имеем, когда произойдет детонация, хотя если поднести устройство к уху, уже можно услышать тихое тиканье.</li>
</ul>
<p style="text-align: right;"><a href="#menu"><em>Вверх</em></a>
